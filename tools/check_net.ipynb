{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import shutil\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils.losses as losses\n",
    "import utils.detectors as detectors\n",
    "import utils.metrics as metrics\n",
    "import utils.optimizer as optim\n",
    "from models.model_builder import getModel\n",
    "from datasets.data_loader import getDataLoader\n",
    "from torchvision import transforms as trn\n",
    "from config import cfg\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = dict()\n",
    "cfg['network_kind'] = 'wrn'\n",
    "cfg['depth'] = 40\n",
    "cfg['widen_factor'] = 2\n",
    "cfg['num_classes'] = 10\n",
    "cfg['drop_rate'] = 0.3\n",
    "model = getModel(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"/home/sr2/Hyeokjun/OOD-saige/results/wrn_cifar10_baseline/ckpt/checkpoint_epoch_100.pyth\", map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Severstal ready.\n"
     ]
    }
   ],
   "source": [
    "cfg['in_dataset'] = dict()\n",
    "cfg['in_dataset']['dataset'] = 'Severstal'\n",
    "cfg['in_dataset']['targets'] = ['ok','1', '2']\n",
    "cfg['in_dataset']['train_transform'] = trn.Compose([trn.RandomHorizontalFlip(),\n",
    "                                         trn.RandomCrop(224),\n",
    "                                         trn.ToTensor()])\n",
    "cfg['in_dataset']['valid_transform'] = trn.Compose([trn.CenterCrop(224),\n",
    "                                         trn.ToTensor()])\n",
    "cfg['in_dataset']['data_root'] = '/home/sr2/HDD2/Openset/'\n",
    "cfg['in_dataset']['split_root'] = '/home/sr2/Hyeokjun/OOD-saige/datasets/data_split/'\n",
    "\n",
    "# DataLoader config\n",
    "cfg['dataloader'] = dict()\n",
    "cfg['dataloader']['batch_size'] = 20\n",
    "cfg['dataloader']['num_workers'] = 1\n",
    "cfg['dataloader']['pin_memory'] = True\n",
    "\n",
    "\n",
    "in_valid_loader = getDataLoader(ds_cfg=cfg['in_dataset'],\n",
    "                                    dl_cfg=cfg['dataloader'],\n",
    "                                    split=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Dataset CIFAR10 ready.\n"
     ]
    }
   ],
   "source": [
    "cfg['in_dataset'] = dict()\n",
    "cfg['in_dataset']['dataset'] = 'cifar10'\n",
    "cfg['in_dataset']['train_transform'] = trn.Compose([trn.RandomHorizontalFlip(),\n",
    "                                         trn.RandomCrop(32),\n",
    "                                         trn.ToTensor()])\n",
    "cfg['in_dataset']['valid_transform'] = trn.Compose([trn.CenterCrop(32),\n",
    "                                         trn.ToTensor()])\n",
    "cfg['in_dataset']['data_root'] = '/home/sr2/HDD2/Openset/'\n",
    "cfg['in_dataset']['split_root'] = '/home/sr2/Hyeokjun/OOD-saige/datasets/data_split/'\n",
    "\n",
    "# DataLoader config\n",
    "cfg['dataloader'] = dict()\n",
    "cfg['dataloader']['batch_size'] = 20\n",
    "cfg['dataloader']['num_workers'] = 1\n",
    "cfg['dataloader']['pin_memory'] = True\n",
    "\n",
    "\n",
    "in_valid_loader = getDataLoader(ds_cfg=cfg['in_dataset'],\n",
    "                                    dl_cfg=cfg['dataloader'],\n",
    "                                    split=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_iterator = iter(in_valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 6, 5, 6, 0, 9, 3, 9, 7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.9564, 0.9998, 0.9994, 0.9998, 0.9999, 0.9995, 1.0000, 0.9919, 0.9986,\n",
      "        0.9989, 0.9999, 0.9998, 0.5427, 0.9985, 0.9982, 0.9998, 1.0000, 0.9981,\n",
      "        0.5206, 0.7564], device='cuda:0', grad_fn=<MaxBackward0>),\n",
      "indices=tensor([4, 6, 5, 6, 0, 9, 3, 9, 7, 6, 9, 8, 2, 3, 8, 8, 7, 7, 5, 3],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "(data, target) = next(dataloader_iterator)\n",
    "data = data.cuda()\n",
    "print(target)\n",
    "logit = model(data)\n",
    "print(torch.max(F.softmax(logit, dim=1),dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 3\n",
    "out_features = 5\n",
    "in_features = 10\n",
    "alpha = 1\n",
    "# weights = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "# bias = nn.Parameter(torch.Tensor(out_features))\n",
    "weights = torch.rand((out_features, in_features))\n",
    "bias = torch.rand((out_features,))\n",
    "features = torch.rand((bs, in_features))\n",
    "\n",
    "def euclidean_distances(features, prototypes, pnorm):\n",
    "    return F.pairwise_distance(features.unsqueeze(2), prototypes.t().unsqueeze(0), p=pnorm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2792, 0.5562, 0.5294, 0.2019, 0.0070, 0.6183, 0.1054, 0.3990, 0.7386,\n",
       "         0.2678],\n",
       "        [0.4292, 0.6393, 0.4971, 0.3767, 0.9255, 0.3906, 0.0946, 0.7144, 0.4681,\n",
       "         0.3511],\n",
       "        [0.2815, 0.5964, 0.9384, 0.5961, 0.6422, 0.8453, 0.6853, 0.9122, 0.9689,\n",
       "         0.5580],\n",
       "        [0.6468, 0.9789, 0.8526, 0.0362, 0.8766, 0.9725, 0.0280, 0.0522, 0.3910,\n",
       "         0.4061],\n",
       "        [0.5719, 0.6909, 0.1939, 0.1932, 0.4687, 0.8602, 0.8138, 0.1008, 0.5467,\n",
       "         0.2539]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "targets = torch.randint(low=0, high=5, size=(bs,))\n",
    "print(targets)\n",
    "targets_one_hot = torch.eye(weights.size(0))[targets]\n",
    "# affine = features.matmul(weights.t()) + bias\n",
    "# intra_inter_affine = torch.where(targets_one_hot != 0, affine, torch.Tensor([float('Inf')]))\n",
    "# intra_affines = intra_inter_affine[intra_inter_affine != float('Inf')]\n",
    "# intra_inter_affine = torch.where(targets_one_hot != 0, torch.Tensor([float('Inf')]), affine)\n",
    "# inter_affines = intra_inter_affine[intra_inter_affine != float('Inf')]\n",
    "# transformed_affines = affine\n",
    "# print(alpha * transformed_affines)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = euclidean_distances(features, weights, 2)\n",
    "intra_inter_distances = torch.where(targets_one_hot != 0, -distances, distances)\n",
    "intra_distances = -intra_inter_distances[intra_inter_distances < 0]\n",
    "inter_distances = intra_inter_distances[intra_inter_distances > 0]\n",
    "transformed_distances = distances\n",
    "regularization = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3094, -1.2153, -1.3058, -1.3996, -1.1051],\n",
       "        [-1.1597, -0.9348, -1.5069, -0.8814, -1.0158],\n",
       "        [-1.3925, -1.3281, -1.1230, -1.7164, -1.2605]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3094,  1.2153,  1.3058,  1.3996,  1.1051],\n",
       "        [ 1.1597, -0.9348,  1.5069,  0.8814,  1.0158],\n",
       "        [ 1.3925, -1.3281,  1.1230,  1.7164,  1.2605]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intra_inter_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3094, 0.9348, 1.3281])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intra_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2153, 1.3058, 1.3996, 1.1051, 1.1597, 1.5069, 0.8814, 1.0158, 1.3925,\n",
       "        1.1230, 1.7164, 1.2605])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def classify_acc_w_ood(logits, targets, confidences, step=1000):\n",
    "    threshold_min = torch.min(confidences)\n",
    "    threshold_max = torch.max(confidences)\n",
    "    threshold_diff = threshold_max - threshold_min\n",
    "    total = logits.size(0) \n",
    "    print(targets.size())\n",
    "    class_correct = (torch.argmax(logits[:len(targets)], dim=1) == targets).float()\n",
    "    \n",
    "    max_threshold = threshold_min\n",
    "    max_acc = -1.\n",
    "    for i in range(step + 1):\n",
    "        threshold = threshold_min + threshold_diff * (i / step)\n",
    "        inliers = (confidences >= threshold).float().data.cpu()\n",
    "        outliers = (confidences < threshold).float().data.cpu()\n",
    "        inlier_correct = (inliers[:len(targets)] * class_correct).sum()\n",
    "        outlier_correct = outliers[len(targets):].sum()\n",
    "        acc = (inlier_correct + outlier_correct) / total\n",
    "        if max_acc < acc:\n",
    "            max_acc = acc\n",
    "            max_threshold = threshold\n",
    "    \n",
    "    return max_acc, max_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7880, 0.3252, 0.4372, 0.8440, 0.8483, 0.3952, 0.9418, 0.1078, 0.7481,\n",
      "        0.0157])\n",
      "tensor([1, 2, 0, 2, 2])\n",
      "tensor([1, 2, 2, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "bs = 10\n",
    "confidences = torch.rand(bs)\n",
    "targets = torch.randint(low=0, high=3, size=(bs//2,))\n",
    "logits = torch.rand((bs, 3))\n",
    "print(confidences)\n",
    "print(targets)\n",
    "print(torch.argmax(logits[:len(targets)], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.6000), tensor(0.7483))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_acc_w_ood(logits, targets, confidences, step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = (torch.argmax(logits[:len(targets)], dim=1) == targets).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 0.])\n",
      "tensor([1., 1., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.7483\n",
    "inliers = (confidences >= threshold).float()\n",
    "outliers = (confidences < threshold).float()\n",
    "print(inliers)\n",
    "print(class_correct)\n",
    "inlier_correct = (class_correct [:] * inliers[:len(targets)]).sum()\n",
    "outlier_correct = outliers[len(targets):].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.), tensor(4.))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inlier_correct, outlier_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from torchvision import datasets, transforms as trn\n",
    "sys.path.append(\"../\")\n",
    "from utils.pgd_attack import LinfPGDAttack\n",
    "from models.wrn import WideResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cfg = dict()\n",
    "m_cfg['depth'] = 10\n",
    "m_cfg['num_classes'] = 10\n",
    "m_cfg['widen_factor'] = 1\n",
    "m_cfg['drop_rate'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideResNet(m_cfg)\n",
    "attack = LinfPGDAttack(model, eps=8.0, nb_iter=2, eps_iter=1.0, rand_init=False, loss_func='OE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = trn.Compose([trn.ToTensor()])\n",
    "dset = datasets.CIFAR10(root=\"/home/sr2/HDD/Hyeokjun/\",\n",
    "                 train='test',\n",
    "                 download=True,\n",
    "                 transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data, target) = dset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = trn.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAJZElEQVR4nAXB2Y8dWX0A4LP8TtWp9W59l97stt1ux4zGHhiDRiYJGfECLyhv+e/CPxBFCEWRIuUBIQUemJFRBpuJ8d7r7bvVvVV1Tp0934d/+o8/q6p1TPwwCndG6XiY7fXziDKIE0Rhvam0DYN+jzijlOq6jifcISdk0+uXKDitNEWMUlrkeZZljHGpdMAEEdBK24Dh5auX1XI55AiP+J4rcDJp/bpxIeBIdFpIZZxfUswhWOspgTiORddar3E3IhQZpRLgjdJrZ9M0w4RhyhAhojPWGAoxJIBRjO6O+Mm0NxkPkzTDGEvVdUYFjKMkQTYEr3rD1JoQscQ5RKNY6c5YnEYxZAmPYotbErxFmGKUZ2nTCmMNwajebYFjWxRwdjgYJZT5rllr54kUlkSo7OcQxdW2BkDDIq13re5a2ZmAcJ5lRkvigMWxcwYoVspELCLeqmaDXIgpst5vWwWDGJI47mXJuGTOO4cQBYoIUd4AAATvlAyU3N5WzrhaCOF0npRIOYo8wYHGXLZdykoIoeu0NNajUDVdJUwjbGcIjPu8YJRzSmhIksRY5xEOQWsbnDY+mOB0gKjWrXNUOG+dr1tzuW4Z8WWDzc1SbsWdvdPJ5AgXW7VZNU27rbvlVn443zoKcDDOysjmaYSDQSjg4JUUBOFR0csyvtsue2VZd+bj5bJRNPLoMAVg8sOqUoEyHHpl8fwHz3bXLojQ22NKQNOQmLHjWTGZTOe7DoZFArqKGaRxqqQx3vb7gxCCdsSYLs3zq4V6+3G7qK2w6G5C//kfvjjaz//t23d/fHNjvQYS6mohGlUUDDnMOYs4TTGzzt45PijWNUyGI7nuCIZGGKktYCqMIwhJo/uDUrvw7uJqvXMBIkpJyd0Ear5WD8vZ9ZDMq1sl9IvXr4n1JitRb4oI9Hpp4UOnTdC7k3EGg73xIE8IYdVuY9qGOOeRDwzynBvE//rudatazmMeQZKlA2q/fTO3GlRvNh5wjEpjO6FlK4K2FhuNMGIEB0IZgFUquACIMMwYQijmLEUZIEIIMcjHSW95U4vl5v6Qqw7xLH304JCozlK2222AbosoGw0ePHh45/2nP33/+jICFUJjLRCIWMS89x5hjAnIzmAjEbJtu9OGWMIbUe9EfXgMwdZ39/CDAyY6fHj2NArdZmuS/git6PFsv2rb+3/3sByk5eDxZlFvtlsWZSTExjvvkTOWYBRCAIddcDaEkPAkL9KrhXx/sQAWovlVN188nLCf/9PDt5fr4nC8N5rdLub9fkY8iwi9XVwCrxbV9eV1w1jaL72UIQDBBHvvCMaYEBcQ9Pu5Bds0XTBuW28/fpo3TZNwcv1+N+XR4eHd/sE9VnvE2dHTn/Cby8QuHOratttPx9p5nOVH2UHRn9Wrm9v5ymDWaYVIyGKuZcMiBnW1Al0zTBBFQKlotoMi62dcbnaTg9Hhk5/95UK/fqOf7w+rSk8fPCVIaLXoB7+7XSXa7A+HlYvZk4Gsrv/nP397cb6gEUMIy4AMIsQYoBg52QSECbIO041Bu10ISu/3sh9//fXRo6/+/df/OstyquXlu7ez+z/go9Ms1GJ9m/iBlmJZi/743mh2IpuSlMhFHSbYGI2tw8FZC4ADcsZgQoCgIA32aDhKZ6n90bOzx8+/2tw2sd3ePzry2M8mY9tZUWltrZHgUP728uK7v3zz/Cs9mo129S1L0d5J5glx2lmlt4tK1Sl466TyUZYDMEr06WzAE3Jy9/jp33+9/+jJn//46zvHg9lnn0fjB5D2RNfIXT2/Ot/ML5wRScH39tj51Yvp/qEVTZAKtxsXZMAhiVk0Y7sYA6OwqYXrcJImlITJKD2/rh786BdHn/8CoYGp217RG5990cLw5Ys/KdnudtXy8hN1mnM4vHf45OzU0ozRPosMdJ34eOmtswQ1lKajbHowAiW7NAbMKSM2OJvk9Ff/8qvnv/x5uTedv/srJbaqt4sP/3dVu9/95jd5wjrVzKa9ssjeX5xrYocHJ2eff4lcvK4uRIc30uIAnfRNCKHpHvcR+KCRd9h6GwzGgcflF19+GTP26s8vNldvlerqzfr8zasmJMx1OdCSZ+NB73p+Y40RdXP+/hNCL5um5hBsPFnZMkl4WiQJxLXYWW8BIe+tBpY66zSy097gv377H8Ppy8n+sRZbxuI8K4HQjLHZZCTrTULj1WJptCt4opvmby++uf7+tbISMeoIzY4ylGkSd9zbAUoef3YPvMcRUA4eERxo5rVZLm+axU1idh7R4WDUPxhbpy6vbgIKhIC2lmKW8dR6RK1HODi9JR7vxEbHsjhQbVLVXnctGZX39yYjQnDM4yQgmyZ8MpoEo0ZF1Iut3s51vRSijsshyUaPnjzzkOhAPIamEd6hiAJnYK19fbH45tXVd2+v13bH+8CiqGlsK0NWjKRwJAKilfIh8jQWRlLqU55kxThKe9PJXr1ZCG3Gx6fCx5/9+KePv3hGgLeNEkJijDHy15dXn97fNEImeToeTnDH8HU2uN074/eO+kdvXt3AdEzMaiWdb1sUiAOAshxFjMl2lzBAGr75wx/uP5pfXNwQgtOYURonSdY2Ukpprc6T+PkPz3hRWmqdEfK8IzWfpMUPzz6b9KffXr+HO8dRD/M352K+CNrFeQ6t2DrfUETWi1Xd2M5sadgW+WB+s75oOx/wdDzC3myqTZzF/V4RUaK0Q8BaRXTDMk9Oj2cHs9H5xXy1EFAOmFyIwYSiLF3OVac1RKXWyBtnnNrKTZbEnehkt9TGOeNCoM1OlGVSlj0pxXK1yfMME4JtiCCJOYoienJ6IkX4/e9f/e/rWwAOvIyGOQGpWOJ3G0COJHzimHeqilJgEFGaquC10SFgHFDQnesQA4aiuNpspDa9fgmEEIgEsvNlvWls3W7/+3ffzwWCpmGI5nnWsSRkMe/1fLOTzW7eCGc6V0QjzphVCoBEBLGYYkzSHAgg62yUQNlP1+u6Dr4cjoTVf/uw+v678+mwnB6liPi9XgEXH5GqeDG2PDG9HA2H0LSiqsRmFW1WiHrqQ3DOIe8IQphgCiAdCRYxb6xYOykcsKoR2qH1Tn54s6pWrW7drDd7fPdwJxE4tmeiZ8orYpe8h/tjPiB2KHy1TqollS04G6FAvPWd7KIookDrzsumY0EXpPBkZwzEWeAs7kf6Pup//jR79OTpyenpT74SF1fN/wMWt9uTtWIfgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FE93F7C4E90>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_inputs = attack.perturb(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAJaklEQVR4nAXBWW9c12EA4LPfc7e5sw93U5QoKVEEyUKCqG5RB3CAtEXTt/6+/oE8FW0e8pAEaREgcZaqRlNali3JJEUOZ7/L2Zd8H/ybH33aLpcUhT7HZwM2GVb9Mc8iRUmJKFputsr4yXCAvNdSCmVYyT3QqmvLfp/AaLSJgWIaq6KfpznFrNMKUhYQMZ0AEML+eFqv73oM3BvwR0fV2eGYpSh6GCETxohGWh8IjRxB5wJEKMlzIXYAhOFwgDCytktQ4bXwFrBeAkMKE0pQqIV3Wic8JymKKAH3+ux0NpiOhkmR4ogbK4yWMEKSJihAH2Q5KYL2jKbekgQlnWutgxnnBKWMJy0klAYAIoKhSPhWq+gUpqSpN4RRV5XkweFglFIMhFgrH0ijHGKg3ytKxtebJiNomHLpO6V0J9oIYi9PbZBEI5jkMUoOoYSaBwIg1O0GREhpMMGtdpIMMEkpHZTlqIAeRe8QYTAFTAPNCCLeWdt5R5bL1mvbmbb2oZcWrfYJQYhilHPdBU4zgK0TQWgBQFwLs22lMkFoRCZjNqAVZ5EliBJunQ8oRuiCjNZY5zX2ClDeqMZGKjwCXnU7v9USkzDcQk2XelUf7D2eTo9w3ukwb7ZyJ9ptbd9cbjwm5GhU9rkpsixaC5CPEEjREQersqpyvm3aqt9vlL28Xm8tZAB8lCECm+3OygjnHg/66cvHL+uFhSIO+tgq0hKU0GR/rxqPJzetIv0qZUZSinhWaW1taMe9QfTReKS1KYri6ka/eb9bNa4G4H6a/PPfPj+aFT979e7ziw8OBgLgrr7RtTZpAspIWIqIyDgO0ZwdnpTrHZn2B3arUGSt0NIYgnFrIwGuM6E3KbzG727vlsI6zBIEMm4OmOS1ftSbzqdgsVx3yn1x8Q1yQec9MJgh4kejLBjcBhvd9mSUk9F0H2UpQMlWbGzTouhBcB7zXlZ4wC6+ebPTu4KnDKE0zyaJ+fzixgUiy4NRlcFgrBEi0V1tnTPYqgABxhRwVAISdId9JAHhmCbQIcqSovQMIACYDYalg/Vd0y4XD4dcC8zT4tH5IZLKJFRsV4TuKp5PBufn94/efvj8L68vKdEwtj4gALOUYeOcAThiQLTy0UoATCdrZ4kBmVLNWmwOD5k13b09+NGUKof3z57waFY7WQ4rwvDx0WS7k2ePz7MR742eLG66XbPCvGQe2xCDQ94HBEP0kdhoow/Yx5yltOx9O2++upoTCDm5Ejer82nykx+ff325Lo/3h9V4uVj0+ikBKQJxvZiDYrvaNjdzRVPew4Ou85BRiLALHsGIEfOBkOGw52onhPFSit3q8t1CdHXO2fz9ekbY4Uf3iuk9vAuOwIOPX7IPNzlYxCA6aSbZyMUAk2wvT9LeuNvc3c4XFqbOdAH4inHTKcIh2W1XXG2oRzCJ2iet2lRp1S+53oXhweyj73168cG8efPVJ/vTdu32z5+gIJTf9K1br1dB+Vm/X3uKn0/k8vJ3v/j51dubyFJKjPTEAhCsR0l0vhPBdcC23reNRAthN1sxyqoffPbp0fn3//iH1yXj0Mnrb75MkmF+8GJ08gKNTtPxg2R0uJS0qM5Pjp+fnD9ChfOZ80R11ijXuOgMCAR6YK2PMaQIYet8QOMiO+qFF88ffOfvPlnfyVztHtw/AN6MD2ZGuICVcUF2DILeV+8/fPGXP/z9SzMaV2u5xpwMj/JAQjTQabVZtbZxJATfacNJz7IUK/PwQLMcnh4eP/3RZ/vn3/3f//q3o+Nq+uwZn9xnrBKmlvNmcXW5mF9B3SYDPh7jt7d/Hh0egnoLZQflIkYboaWUTvZgiyGBmGwa4RXMew4jPJ3wy5v6/k9/fPLkH0iofCerajR58EKS3qtXf7RNt63X2+sP2Cuekf14/OK7D11M01gEXgHnxOXcKR+Ys5Fms3JylBLfqV5OIMEQAewl5PRf/vWfPvnHz7LRZPHuS4zMstks3v3f3Q7+5t9/1ssT0YmD/bKo0rffXitixkcnD558DGC+Xtya1m+kip6oDrcWRtU+7R+SgA0IHjoAogzQs5K/+PiHCU1f/8+rze2XnTbNbnn9+qKNCY6GA5xVyWhQXt/cGi3bRlx+/S0A47ZuOLcmn65cP0lZkac5z3di7YwjDqCgHEg9VFERNytH//nz/xhPXo1nx040NEl6SY8wWsL0aDiQTUOTZLXYxuCrLHe1+P8v/nT95rV0GlAIA80PclAqlLjM7EYkPf/eMUHBM0I5A47EDJbB+O3dW3VzlX+nNgBPy1FxMAlavr+7sQgGgpQzCcQJ67kQoAmUeGs2KGAhakVtuaebstm5lelAVZxO92YkxJSl1AaTJ1k1GAovqwox5MRuEQBvKZgd3muMOx+fzOe/clFQH2sp+hXJINfUCKW+Xmy22+BjN3iIDocJjaipXZAw3591tUYZAsZaBHmgSa01jqHI0rycMV7N9oa73UoIvX981oL02Q9++OjFC8R4I7yoJYQegnB9c/vh7bJtO1oUk/GYqhRfV9WH8UN+/3S8983rJRntM73aaBu6DkVSc0R6cQIh1rI1FAEFfv/b359u5ldXS0RgD0MIkzJJGiWllMaFXk4/efqQDbIAndVGXm9Aw8csffb4ybh/+Lub35KzfTby7PW1Wsyj8Cnsg53YaFDjDi3nadMq43cUbnrZaH67vuoEsnA0mw2cXG13aQWG5QGDQBqHEtpIo1RSOvD40dHe3uzm3c1uIVA2wShtq7GaneG00Ea0wAXnYCettttGz0kCVWvulrfCWiit8b6ta0hYr1/hkNytFrVRIQDvPCN5znleVUdnJ7q2v/zviz9dLAkniBd8mgEuO5iEdkdARCmZemqk3LKMJJBAzA0MTmgPMYUIO2MFSQhGBG93Rqv1oN8jLAsE1obKddPUoem6X//mYi4caduU4IJXCqY4ybnYBSHkdn2jpLdtrLIJwdhoizjiwFGKI0SsQIQ542HCWK/ItutNA0EBE0nh229XF3++nA2L0WnmQBhXObl6D7qWlSNX5DotkvGQtLu26Kndhi1AiAgGQCMMUCCAAAGQciYtECFQZ03cRSE8Sddto3zYtvbd69V2pVQH96q9J8ez1iJi02EUL7XrkNv2cl/N+GDf9VWol3k1g7Zj3nHgkQnOdYJkGUWwMUDWikNTknIV1tKSPB9mNA1cnITi6bP8/PnT09OH33+pbu/qvwKDDdrN7djihwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FE93F348B90>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans(adv_inputs.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0078)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inputs - adv_inputs).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ood",
   "language": "python",
   "name": "ood"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
